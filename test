import torch
import numpy as np
import sounddevice as sd
import queue
import whisper
from threading import Thread
from transformers import AutoTokenizer, AutoModelForCausalLM

# === Voice Activity Detector ===
class VoiceActivityDetector:
    def __init__(self, sample_rate=16000, threshold=0.5):
        self.sample_rate = sample_rate
        self.threshold = threshold
        self.model, self.utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            trust_repo=True,
            onnx=False
        )
        self.get_speech_ts = self.utils[0]

    def is_speech(self, audio_chunk, return_probs=False):
        if not isinstance(audio_chunk, torch.Tensor):
            audio_tensor = torch.from_numpy(audio_chunk).float()
        else:
            audio_tensor = audio_chunk.float()

        if len(audio_tensor.shape) == 1:
            audio_tensor = audio_tensor.unsqueeze(0)

        prob = self.model(audio_tensor, self.sample_rate).item()

        if return_probs:
            return prob
        return prob > self.threshold


# === Constants ===
SAMPLE_RATE = 16000
FRAME_SIZE = 512  # Must be exactly 512 for Silero at 16kHz
FRAME_DURATION_MS = int(FRAME_SIZE / SAMPLE_RATE * 1000)
CHANNELS = 1

# === Models and Buffers ===
vad = VoiceActivityDetector(sample_rate=SAMPLE_RATE)
whisper_model = whisper.load_model("base")
audio_q = queue.Queue()
speech_audio = []

# === Load LLaMA (Hugging Face Transformers) ===
# print("Loading LLaMA model...")
# llama_model_id = "meta-llama/Llama-2-7b-chat-hf"  # Replace with your preferred model
# tokenizer = AutoTokenizer.from_pretrained(llama_model_id, use_fast=True)
# llm_model = AutoModelForCausalLM.from_pretrained(llama_model_id).to("cuda" if torch.cuda.is_available() else "cpu")

# def query_llama(prompt):
#     inputs = tokenizer(prompt, return_tensors="pt").to(llm_model.device)
#     with torch.no_grad():
#         outputs = llm_model.generate(**inputs, max_new_tokens=150, do_sample=True, top_p=0.9)
#     return tokenizer.decode(outputs[0], skip_special_tokens=True)

# === Audio Processing ===
def audio_callback(indata, frames, time, status):
    if status:
        print(f"Sounddevice status: {status}")
    audio_q.put(indata.copy())

def vad_loop():
    silence_count = 0
    speaking = False

    while True:
        audio_frame = audio_q.get()
        audio_np = audio_frame[:, 0]  # Mono

        is_speech = vad.is_speech(audio_np)

        if is_speech:
            speaking = True
            speech_audio.append(audio_np)
            silence_count = 0
        elif speaking:
            silence_count += 1
            if silence_count > int(1000 / FRAME_DURATION_MS):  # ~1s of silence
                print("Processing speech...")

                try:
                    audio_array = np.concatenate(speech_audio)
                    audio_array = whisper.pad_or_trim(audio_array)
                    result = whisper_model.transcribe(audio_array, fp16=False)
                    transcription = result['text']
                    print(f"User said: {transcription}")

                    # Send to LLaMA
                    # response = query_llama(transcription)
                    print(f"LLaMA response: {response}")

                except Exception as e:
                    print(f"Error during transcription or LLM response: {e}")

                speech_audio.clear()
                speaking = False

def main():
    print("Listening for speech...")
    with sd.InputStream(callback=audio_callback,
                        channels=CHANNELS,
                        samplerate=SAMPLE_RATE,
                        blocksize=FRAME_SIZE):
        vad_thread = Thread(target=vad_loop, daemon=True)
        vad_thread.start()
        vad_thread.join()

if __name__ == "__main__":
    main()
